import json

def load_json(filename):
    with open(filename, "rb") as f:
        results = json.load(f)
    return results

def write_json(obj, filename):
    with open(filename, "w") as f:
        json.dump(obj, f, indent=4)

def load_txt(filename):
    with open(filename, 'r') as f:
        results = [s.strip('\n').strip() for s in f.readlines()]
    return results

def remove_special_tokens(response: str):
    if not isinstance(response, str):
        response = str(response)
    special_tokens = ["-", "/", "\\", ",", ".", " ", "&", "#", "(", ")", "+", '\n']
    for st in special_tokens:
        response = response.replace(st, "")
    return response.lower()

def generate_log(prompts, outputs):
    assert len(prompts) == len(outputs), "Length of prompts and outputs must match."
    log = []
    for i, (prompt, output) in enumerate(zip(prompts, outputs)):
        entry = {"prompt": prompt, "output": output}
        log.append(entry)
    return log

def load_log(file):
    results = load_json(file)
    prompts = [item["prompt"] for item in results]
    outputs = [item["output"] for item in results]
    return prompts, outputs


"""Utilities for calling LLM APIs.
"""

import multiprocessing
import os
import time
import warnings
import copy
from typing import Union

import openai
from datasets import Dataset, concatenate_datasets


def generate(
    prompt: Union[str, list[str]],
    model: str,
    system_prompt: str = None,
    num_retries: int = 5,
    delay: int = 0,
    **kwargs,
) -> str:
    """Call an LLM to generate a response.

    Currently supported models are:

    - Any OpenAI model (requires an OpenAI API key)
    - Llama-3.1 70B (requires a Perplexity API key)
    - Dolphin-2.6 Mixtral-8x7B (requires a DeepInfra API key)

    Args:
        prompt (Union[str, list[str]]): Prompt to respond to. If this is a list, it will be converted into a conversating with alternating "user" and "assistant" roles.
        model (str): Model.
        system_prompt (str, optional): System prompt. Defaults to None.
        num_retries (int, optional): Number of retries if an error is encountered (e.g., rate limit). Defaults to 5.
        delay (int, optional): Initial delay before calling the API. Defaults to 0.

    Returns:
        str: Response.
    """
    if num_retries == 0:
        msg = f"Failed to get response from model {model} for prompt {prompt}"
        warnings.warn(msg)
        return ""

    if delay > 0:
        time.sleep(delay)

    # construct messages
    messages = []
    if system_prompt is not None:
        messages.append({"role": "system", "content": system_prompt})

    if isinstance(prompt, str):
        messages.append({"role": "user", "content": prompt})
    else:
        # prompt is a list of alternating user and assistant messages
        for i, content in enumerate(prompt):
            messages.append({"role": "user" if i % 2 == 0 else "assistant", "content": content})

    try:
        model_kwargs = {
            "llama-3.1-70b-instruct": {
                "api_key": os.getenv("PPLX_API_KEY"),
                "base_url": "https://api.perplexity.ai",
            },
            "meta-llama/Meta-Llama-3.1-70B-Instruct": {
                "api_key": os.getenv("DEEPINFRA_API_KEY"),
                "base_url": "https://api.deepinfra.com/v1/openai",
            },
            "cognitivecomputations/dolphin-2.6-mixtral-8x7b": {
                "api_key": os.getenv("DEEPINFRA_API_KEY"),
                "base_url": "https://api.deepinfra.com/v1/openai",
            },
            'gpt-4o':{
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_API_BASE"),
            },
            'gpt-4o-mini':{
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_API_BASE"),
            },
            'gpt-3.5-turbo':{
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_API_BASE"),
            },
        }
        assert model in model_kwargs, f"The model for evaluation must be in the model list {model_kwargs.keys()}"
        with openai.OpenAI(**model_kwargs.get(model, {})) as client:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs,
            )
            return response.choices[0].message.content
        
            
    except Exception as e:
        print(e)
        return generate(
            prompt,
            model,
            system_prompt=system_prompt,
            num_retries=num_retries - 1,
            delay=2 * max(delay, 1),
            **kwargs,
        )


def generate_to_dataset(
    dataset: Dataset,
    model: str,
    target_column: str = "prompt",
    **kwargs,
) -> Dataset:
    """Generate responses to a HuggingFace dataset of prompts.

    Args:
        dataset (Dataset): Dataset with prompts.
        models (list[str]): Models used to generate responses.
        target_column (str, optional): Column that the models should use as the prompt. Defaults to "prompt".
        decode_responses (bool, optional): Decode the raw responses. Defaults to True.

    Returns:
        Dataset: Dataset with responses.
    """
    generated_datasets = []
    num_proc = min(multiprocessing.cpu_count(), 20)
    generated_dataset = dataset.map(
        lambda x: {"response": generate(x[target_column], model, **kwargs)},
        num_proc=num_proc,
        load_from_cache_file=kwargs.pop('load_from_cache_file', False),
    )
    
    return generated_dataset
